{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "738e729d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as mcolors\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "import os\n",
    "import pickle\n",
    "import random\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "from tqdm import tnrange\n",
    "from time import time\n",
    "\n",
    "from sklearn.covariance import MinCovDet\n",
    "from scipy.stats import chi2\n",
    "\n",
    "\n",
    "from scipy.stats import multivariate_normal\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, confusion_matrix, r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import preprocessing, linear_model\n",
    "from tensorflow.keras import layers, losses\n",
    "from tensorflow.keras.datasets import fashion_mnist\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.regularizers import l1, l2, l1_l2\n",
    "from sklearn.preprocessing import minmax_scale, OneHotEncoder\n",
    "from keras.layers.convolutional import Conv1D\n",
    "from keras.layers.convolutional import MaxPooling1D\n",
    "from keras.layers import Input, Dense, Reshape, Concatenate, Flatten, Lambda, Reshape, Dropout\n",
    "from keras.losses import MeanSquaredError as mse\n",
    "from tensorflow.keras import backend as K\n",
    "from keras.optimizers import Adam\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5dfc24e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['fall_simulation',\n",
       " 'fall_simulation2',\n",
       " 'fall_simulation3',\n",
       " 'fall_simulation_2.csv',\n",
       " 'fall_simulation_3.csv',\n",
       " 'trial_data_1',\n",
       " 'trial_data_1.csv',\n",
       " 'trial_data_2',\n",
       " 'trial_data_2.csv',\n",
       " 'trial_data_3',\n",
       " 'trial_data_3.csv',\n",
       " 'trial_data_4',\n",
       " 'trial_data_4.csv',\n",
       " 'trial_data_5',\n",
       " 'trial_data_5.csv']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DATASETS_PATH = \"C:/Users/ernest.liu/Documents/git/Morphine-22-23/ML/Datasets/19-01-2023/\"\n",
    "os.chdir(DATASETS_PATH)\n",
    "os.listdir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bdf4816e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['trial_data_1.csv', 'trial_data_2.csv', 'trial_data_3.csv', 'trial_data_4.csv', 'trial_data_5.csv']\n",
      "['fall_simulation_2.csv', 'fall_simulation_3.csv']\n"
     ]
    }
   ],
   "source": [
    "adl_files = [file_name for file_name in os.listdir() if \".csv\" in file_name and \"trial\" in file_name]\n",
    "fall_files = [file_name for file_name in os.listdir() if \".csv\" in file_name and \"fall\" in file_name]\n",
    "\n",
    "print(adl_files)\n",
    "print(fall_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2dd65722",
   "metadata": {},
   "outputs": [],
   "source": [
    "trial_data_1 = pd.read_csv(DATASETS_PATH+'trial_data_1.csv').iloc[:,1:]\n",
    "trial_data_2 = pd.read_csv(DATASETS_PATH+'trial_data_2.csv').iloc[:,1:]\n",
    "trial_data_3 = pd.read_csv(DATASETS_PATH+'trial_data_3.csv').iloc[:,1:]\n",
    "trial_data_4 = pd.read_csv(DATASETS_PATH+'trial_data_4.csv').iloc[:,1:]\n",
    "trial_data_5 = pd.read_csv(DATASETS_PATH+'trial_data_5.csv').iloc[:,1:]\n",
    "\n",
    "fall_simulation_2 = pd.read_csv(DATASETS_PATH+'fall_simulation_2.csv').iloc[:,1:]\n",
    "fall_simulation_3 = pd.read_csv(DATASETS_PATH+'fall_simulation_3.csv').iloc[:,1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "189b1c5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only take in accelerometer and gyroscope data\n",
    "trial_data_1 = trial_data_1[['Ax','Ay','Az','gx','gy','gz']]\n",
    "trial_data_2 = trial_data_2[['Ax','Ay','Az','gx','gy','gz']]\n",
    "trial_data_3 = trial_data_3[['Ax','Ay','Az','gx','gy','gz']]\n",
    "trial_data_4 = trial_data_4[['Ax','Ay','Az','gx','gy','gz']]\n",
    "trial_data_5 = trial_data_5[['Ax','Ay','Az','gx','gy','gz']]\n",
    "\n",
    "fall_simulation_2 = fall_simulation_2[['Ax','Ay','Az','gx','gy','gz']]\n",
    "fall_simulation_3 = fall_simulation_3[['Ax','Ay','Az','gx','gy','gz']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e683ce6c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Ax</th>\n",
       "      <th>Ay</th>\n",
       "      <th>Az</th>\n",
       "      <th>gx</th>\n",
       "      <th>gy</th>\n",
       "      <th>gz</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Ax  Ay  Az  gx  gy  gz\n",
       "0   0   0   0   0   0   0"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Pivoting datasets to have 20 sets of data per row\n",
    "pd.DataFrame([[0,0,0,0,0,0]], columns = ['Ax','Ay','Az','gx','gy','gz'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "98902082",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Ax</th>\n",
       "      <th>Ay</th>\n",
       "      <th>Az</th>\n",
       "      <th>gx</th>\n",
       "      <th>gy</th>\n",
       "      <th>gz</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3.35</td>\n",
       "      <td>8.10</td>\n",
       "      <td>-3.17</td>\n",
       "      <td>1.02</td>\n",
       "      <td>-0.56</td>\n",
       "      <td>0.21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3.62</td>\n",
       "      <td>8.19</td>\n",
       "      <td>-3.34</td>\n",
       "      <td>1.00</td>\n",
       "      <td>-0.29</td>\n",
       "      <td>0.15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3.75</td>\n",
       "      <td>8.35</td>\n",
       "      <td>-3.35</td>\n",
       "      <td>0.97</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3.97</td>\n",
       "      <td>8.43</td>\n",
       "      <td>-3.72</td>\n",
       "      <td>0.93</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71936</th>\n",
       "      <td>-0.50</td>\n",
       "      <td>10.55</td>\n",
       "      <td>0.44</td>\n",
       "      <td>0.73</td>\n",
       "      <td>0.05</td>\n",
       "      <td>-0.07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71937</th>\n",
       "      <td>-0.50</td>\n",
       "      <td>10.47</td>\n",
       "      <td>0.46</td>\n",
       "      <td>0.76</td>\n",
       "      <td>0.05</td>\n",
       "      <td>-0.08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71938</th>\n",
       "      <td>-0.50</td>\n",
       "      <td>10.39</td>\n",
       "      <td>0.62</td>\n",
       "      <td>0.77</td>\n",
       "      <td>0.06</td>\n",
       "      <td>-0.08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71939</th>\n",
       "      <td>-0.50</td>\n",
       "      <td>10.30</td>\n",
       "      <td>0.56</td>\n",
       "      <td>0.79</td>\n",
       "      <td>0.06</td>\n",
       "      <td>-0.09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71940</th>\n",
       "      <td>-0.45</td>\n",
       "      <td>10.18</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.81</td>\n",
       "      <td>0.05</td>\n",
       "      <td>-0.09</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>71941 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         Ax     Ay    Az    gx    gy    gz\n",
       "0      0.00   0.00  0.00  0.00  0.00  0.00\n",
       "1      3.35   8.10 -3.17  1.02 -0.56  0.21\n",
       "2      3.62   8.19 -3.34  1.00 -0.29  0.15\n",
       "3      3.75   8.35 -3.35  0.97  0.00  0.09\n",
       "4      3.97   8.43 -3.72  0.93  0.31  0.03\n",
       "...     ...    ...   ...   ...   ...   ...\n",
       "71936 -0.50  10.55  0.44  0.73  0.05 -0.07\n",
       "71937 -0.50  10.47  0.46  0.76  0.05 -0.08\n",
       "71938 -0.50  10.39  0.62  0.77  0.06 -0.08\n",
       "71939 -0.50  10.30  0.56  0.79  0.06 -0.09\n",
       "71940 -0.45  10.18  0.50  0.81  0.05 -0.09\n",
       "\n",
       "[71941 rows x 6 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_new_rows = 1\n",
    "new_rows = pd.DataFrame([[0,0,0,0,0,0] for i in range(num_new_rows)], columns = ['Ax','Ay','Az','gx','gy','gz'])\n",
    "pd.concat([\n",
    "    new_rows, trial_data_1\n",
    "], axis=0).\\\n",
    "reset_index(drop=True).rename({\n",
    "    'Ax' : f'Ax{num_new_rows}',\n",
    "    'Ay' : f'Ay{num_new_rows}',\n",
    "    'Az' : f'Az{num_new_rows}',\n",
    "    'gx' : f'gx{num_new_rows}',\n",
    "    'gy' : f'gy{num_new_rows}',\n",
    "    'gz' : f'gz{num_new_rows}',\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "84833c6a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Ax</th>\n",
       "      <th>Ay</th>\n",
       "      <th>Az</th>\n",
       "      <th>gx</th>\n",
       "      <th>gy</th>\n",
       "      <th>gz</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Ax  Ay  Az  gx  gy  gz\n",
       "0   0   0   0   0   0   0"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame([[0,0,0,0,0,0] for i in range(num_new_rows)], columns = ['Ax','Ay','Az','gx','gy','gz'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9bdaba0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aadf9b16",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15131c81",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c2d15d2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12ec6b2b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63007455",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.concat([trial_data_1, trial_data_2, trial_data_3, trial_data_4, trial_data_5]).reset_index(drop=True)\n",
    "test_data = pd.concat([fall_simulation_2, fall_simulation_3]).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c8bf526",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EDA\n",
    "plt.hist(np.abs(train_data.Ay), bins=100, alpha=0.5, density=True)\n",
    "plt.hist(np.abs(test_data.Ay), bins=100, alpha=0.5, density=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ce73b76",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "010570b1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a73180f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e37a0e4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = preprocessing.MinMaxScaler()\n",
    "scaler.fit(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7044129",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_norm_presplit = scaler.transform(train_data)\n",
    "test_data_norm = scaler.transform(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da93a2f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_ratio = 0.2\n",
    "valid_indices = sorted(random.sample(list(range(train_data_norm_presplit.shape[0])), round(train_data_norm_presplit.shape[0] * valid_ratio)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4601de0",
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_data = train_data.iloc[valid_indices, :].reset_index(drop = True)\n",
    "valid_data_norm = train_data_norm_presplit[valid_indices]\n",
    "train_data_norm = np.delete(train_data_norm_presplit, valid_indices, axis = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50264422",
   "metadata": {},
   "source": [
    "## Mahalanobis Distance Outlier Removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "881a3204",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Covariance matrix\n",
    "# covariance = np.cov(train_data, rowvar=False)\n",
    "\n",
    "start_time = time()\n",
    "cov = MinCovDet().fit(train_data_norm)\n",
    "end_time = time()\n",
    "print(f\"Time elapsed: {(end_time - start_time)/60} minutes\")\n",
    "covariance = cov.covariance_\n",
    "\n",
    "covariance = np.cov(train_data_norm, rowvar=False)\n",
    "\n",
    "# Covariance matrix power of -1\n",
    "covariance_pm1 = np.linalg.matrix_power(covariance, -1)\n",
    "\n",
    "# Center point\n",
    "centerpoint = np.mean(train_data_norm, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ef4aaf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distances between center point and \n",
    "distances = []\n",
    "for i, val in enumerate(train_data_norm):\n",
    "    p1 = val\n",
    "    p2 = centerpoint\n",
    "    distance = (p1-p2).T.dot(covariance_pm1).dot(p1-p2)\n",
    "    distances.append(distance)\n",
    "distances = np.array(distances)\n",
    "\n",
    "# Cutoff (threshold) value from Chi-Sqaure Distribution for detecting outliers \n",
    "cutoff_threshold = 0.99\n",
    "cutoff = chi2.ppf(cutoff_threshold, train_data_norm.shape[1])\n",
    "\n",
    "# Index of outliers\n",
    "outlierIndexes = np.where(distances > cutoff)\n",
    "\n",
    "print('--- Index of Outliers ----')\n",
    "print(outlierIndexes)\n",
    "print(f\"There are {len(outlierIndexes[0])} outliers identified\")\n",
    "\n",
    "print('--- Observations found as outlier -----')\n",
    "print(train_data_norm[distances > cutoff, :])\n",
    "\n",
    "train_data_norm = np.delete(train_data_norm, outlierIndexes, axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37b3abc5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37d0de86",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2879cdbc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8342416",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2bb8b713",
   "metadata": {},
   "source": [
    "## Building Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0033e4da",
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_space_dim = 2\n",
    "def sampling(args):\n",
    "    z_mean, z_log_sigma = args\n",
    "    epsilon = K.random_normal(shape=(latent_space_dim,), mean=0., stddev=1.) # stddev is set to 0.1 in this post: https://blog.keras.io/building-autoencoders-in-keras.html\n",
    "    return_value = z_mean + K.exp(z_log_sigma) * epsilon\n",
    "    #print(z_mean)\n",
    "    \n",
    "    return return_value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ad4437c",
   "metadata": {},
   "source": [
    "### Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8279e70d",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = Input((6,))\n",
    "layer1 = Dense(6, activation = 'relu')(inputs)\n",
    "layer2 = Dense(200, activation = 'relu')(layer1) # Changed from 3\n",
    "layer3 = Dense(100, activation = 'relu')(layer2) # Changed from 3\n",
    "layer4 = Dense(60, activation = 'relu')(layer3)\n",
    "layer5 = Dense(30, activation = 'relu')(layer4)\n",
    "z_mean = Dense(latent_space_dim)(layer5)\n",
    "z_log_sigma = Dense(latent_space_dim)(layer5)\n",
    "encoder = keras.Model(inputs, [z_mean, z_log_sigma], name=\"encoder\")\n",
    "encoder.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e9099e7",
   "metadata": {},
   "source": [
    "### Sampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94dc8098",
   "metadata": {},
   "outputs": [],
   "source": [
    "sampler_input1 = keras.Input(shape=(latent_space_dim,))\n",
    "sampler_input2 = keras.Input(shape=(latent_space_dim,))\n",
    "latent_sample = Lambda(sampling, output_shape = (latent_space_dim,))([sampler_input1, sampler_input2])\n",
    "sampler = keras.Model([sampler_input1, sampler_input2], latent_sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7804bf1",
   "metadata": {},
   "source": [
    "### Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5cf6b9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_inputs = keras.Input(shape=(latent_space_dim,))\n",
    "layer6 = Dense(20, activation = 'relu')(latent_inputs)\n",
    "layer7 = Dense(30, activation = 'relu')(layer6)\n",
    "layer8 = Dense(50, activation = 'relu')(layer7)\n",
    "layer9 = Dense(100, activation = 'relu')(layer8)\n",
    "layer10 = Dense(200, activation = 'relu')(layer9)\n",
    "decoder_outputs = Dense(6, activation = 'sigmoid')(layer10)\n",
    "decoder = keras.Model(latent_inputs, decoder_outputs, name=\"decoder\")\n",
    "decoder.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1508701b",
   "metadata": {},
   "source": [
    "## VAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79f9e962",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAE(Model):\n",
    "    def __init__(self, encoder, sampler, decoder, beta = 1, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.encoder = encoder\n",
    "        self.sampler = sampler\n",
    "        self.decoder = decoder\n",
    "        self.beta = beta\n",
    "        self.total_loss_tracker = keras.metrics.Mean(name=\"total_loss\")\n",
    "        self.reconstruction_loss_tracker = keras.metrics.Mean(\n",
    "            name=\"reconstruction_loss\"\n",
    "        )\n",
    "        self.kl_loss_tracker = keras.metrics.Mean(name=\"kl_loss\")\n",
    "\n",
    "    @property\n",
    "    def metrics(self):\n",
    "        return [\n",
    "            self.total_loss_tracker,\n",
    "            self.reconstruction_loss_tracker,\n",
    "            self.kl_loss_tracker,\n",
    "        ]\n",
    "\n",
    "    def train_step(self, data):\n",
    "        data = data[0]\n",
    "        with tf.GradientTape() as tape:\n",
    "            z_mean, z_log_sigma = self.encoder(data)\n",
    "            z_log_var = 2 * z_log_sigma\n",
    "            z = self.sampler([z_mean, z_log_var])\n",
    "            reconstruction = self.decoder(z)\n",
    "            reconstruction_loss = tf.reduce_mean(keras.losses.binary_crossentropy(data, reconstruction))\n",
    "            kl_loss = -0.5 * (1 + z_log_var - tf.square(z_mean) - tf.exp(z_log_var))\n",
    "            kl_loss = tf.reduce_mean(kl_loss)\n",
    "            total_loss = reconstruction_loss + self.beta * kl_loss\n",
    "        grads = tape.gradient(total_loss, self.trainable_weights)\n",
    "        self.optimizer.apply_gradients(zip(grads, self.trainable_weights))\n",
    "        self.total_loss_tracker.update_state(total_loss)\n",
    "        self.reconstruction_loss_tracker.update_state(reconstruction_loss)\n",
    "        self.kl_loss_tracker.update_state(kl_loss)\n",
    "        return {\n",
    "            \"loss\": self.total_loss_tracker.result(),\n",
    "            \"reconstruction_loss\": self.reconstruction_loss_tracker.result(),\n",
    "            \"kl_loss\": self.kl_loss_tracker.result(),\n",
    "        }\n",
    "    def test_step(self, data):\n",
    "        if isinstance(data, tuple):\n",
    "            #print(f\"Ran. data: {data}\")\n",
    "            data = data[0]\n",
    "\n",
    "        z_mean, z_log_sigma = self.encoder(data)\n",
    "        z_log_var = 2 * z_log_sigma\n",
    "        z = self.sampler([z_mean, z_log_var])\n",
    "        reconstruction = self.decoder(z)\n",
    "        reconstruction_loss = tf.reduce_mean(keras.losses.binary_crossentropy(data, reconstruction))\n",
    "        kl_loss = -0.5 * (1 + z_log_var - tf.square(z_mean) - tf.exp(z_log_var))\n",
    "        kl_loss = tf.reduce_mean(kl_loss)\n",
    "        total_loss = reconstruction_loss + self.beta * kl_loss\n",
    "        return {\n",
    "            \"loss\": total_loss,\n",
    "            \"reconstruction_loss\": reconstruction_loss,\n",
    "            \"kl_loss\": kl_loss,\n",
    "        }\n",
    "    \n",
    "    \n",
    "    def call(self, data):\n",
    "        z_mean, z_log_sigma = self.encoder(data)\n",
    "        z_log_var = 2 * z_log_sigma\n",
    "        z = self.sampler([z_mean, z_log_var])\n",
    "        reconstruction = self.decoder(z)\n",
    "        reconstruction_loss = tf.reduce_mean(keras.losses.binary_crossentropy(data, reconstruction))\n",
    "        kl_loss = -0.5 * (1 + z_log_var - tf.square(z_mean) - tf.exp(z_log_var))\n",
    "        kl_loss = tf.reduce_mean(kl_loss)\n",
    "        total_loss = reconstruction_loss + self.beta * kl_loss\n",
    "        self.add_metric(kl_loss, name='kl_loss', aggregation='mean')\n",
    "        self.add_metric(total_loss, name='total_loss', aggregation='mean')\n",
    "        self.add_metric(reconstruction_loss, name='reconstruction_loss', aggregation='mean')\n",
    "        return reconstruction\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3deded5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "vae = VAE(encoder, sampler, decoder, beta = 0.01)\n",
    "vae.compile(optimizer = 'adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c79db1a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "callback = EarlyStopping(\n",
    "    monitor=\"val_loss\",\n",
    "    patience=5\n",
    ")\n",
    "\n",
    "history = vae.fit(x = train_data_norm, y = train_data_norm, # Changed from train_data_norm \n",
    "                  epochs = 1000,\n",
    "                  shuffle = True,\n",
    "                  batch_size = 32,\n",
    "                  workers = 8,\n",
    "                  validation_data = (valid_data_norm, valid_data_norm), # Changed from valid_data_norm\n",
    "                  callbacks = [callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cedf11a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history['loss'], label = 'Training Loss')\n",
    "plt.plot(history.history['val_loss'], label = 'Validation Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "036ea548",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_predictions = vae.predict(train_data_norm)\n",
    "test_predictions = vae.predict(test_data_norm)\n",
    "valid_predictions = vae.predict(valid_data_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6686659",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"vae_fixed_loss_beta_0_01\"\n",
    "# os.chdir(\"C:/Users/ernest.liu/Documents/git/Morphine-22-23/ML/Model/weights/\")\n",
    "# vae.save_weights(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e0c62db",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Training MAE: {np.sum(np.abs(train_data_norm - train_predictions))/(train_data_norm.shape[0] * train_data_norm.shape[1])}\")\n",
    "print(f\"Testing MAE (Anomalous Data): {np.sum(np.abs(test_data_norm - test_predictions))/(test_data_norm.shape[0] * test_data_norm.shape[1])}\")\n",
    "print(f\"Validation MAE (Normal Data): {np.sum(np.abs(valid_data_norm - valid_predictions))/(valid_data_norm.shape[0] * valid_data_norm.shape[1])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2186634",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_mae = np.sum(np.abs(train_data_norm - train_predictions), axis = 1)\n",
    "plt.hist(train_mae, bins = 20)\n",
    "plt.title(\"Training Dataset\")\n",
    "plt.xlabel(\"Reconstruction Error (MAE)\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44d99ec7",
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_mae = np.sum(np.abs(valid_data_norm - valid_predictions), axis = 1)\n",
    "plt.hist(valid_mae, bins = 20)\n",
    "plt.title(\"Validation (Normal Samples) Dataset\")\n",
    "plt.xlabel(\"Reconstruction Error (MAE)\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "995dbc41",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_mae = np.sum(np.abs(test_data_norm - test_predictions), axis = 1)\n",
    "plt.hist(test_mae, bins = 20)\n",
    "plt.title(\"Testing (Anomalous) Dataset\")\n",
    "plt.xlabel(\"Reconstruction Error (MAE)\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0e64acf",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_threshold = np.quantile(valid_mae, 0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73ea2963",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "110df665",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data[\"mae\"] = np.sum(np.abs(scaler.transform(train_data) - vae.predict(scaler.transform(train_data))), axis = 1)\n",
    "train_data[\"is_anomalous\"] = train_data.mae > best_threshold\n",
    "\n",
    "test_data[\"mae\"] = test_mae\n",
    "test_data[\"is_anomalous\"] = test_mae > best_threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a807a7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b18d59a",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(test_data.index, test_data.mae, s = 0.2)\n",
    "plt.axhline(y=best_threshold, color='r', linestyle='-')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0613ee69",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(train_data.index, train_data.mae, s = 0.2)\n",
    "plt.axhline(y=best_threshold, color='r', linestyle='-')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d02c1f15",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum(test_data.is_anomalous)/test_data.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b34137a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum(train_data.is_anomalous)/train_data.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcff53e8",
   "metadata": {},
   "source": [
    "## Loading Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c33529fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_space_dim = 2\n",
    "def sampling(args):\n",
    "    z_mean, z_log_sigma = args\n",
    "    epsilon = K.random_normal(shape=(latent_space_dim,), mean=0., stddev=1.) # stddev is set to 0.1 in this post: https://blog.keras.io/building-autoencoders-in-keras.html\n",
    "    return_value = z_mean + K.exp(z_log_sigma) * epsilon\n",
    "    #print(z_mean)\n",
    "    \n",
    "    return return_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56b34417",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = Input((6,))\n",
    "layer1 = Dense(6, activation = 'relu')(inputs)\n",
    "layer2 = Dense(200, activation = 'relu')(layer1) # Changed from 3\n",
    "layer3 = Dense(100, activation = 'relu')(layer2) # Changed from 3\n",
    "layer4 = Dense(60, activation = 'relu')(layer3)\n",
    "layer5 = Dense(30, activation = 'relu')(layer4)\n",
    "z_mean = Dense(latent_space_dim)(layer5)\n",
    "z_log_sigma = Dense(latent_space_dim)(layer5)\n",
    "encoder = keras.Model(inputs, [z_mean, z_log_sigma], name=\"encoder\")\n",
    "encoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55c2c6d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "sampler_input1 = keras.Input(shape=(latent_space_dim,))\n",
    "sampler_input2 = keras.Input(shape=(latent_space_dim,))\n",
    "latent_sample = Lambda(sampling, output_shape = (latent_space_dim,))([sampler_input1, sampler_input2])\n",
    "sampler = keras.Model([sampler_input1, sampler_input2], latent_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82494e5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_inputs = keras.Input(shape=(latent_space_dim,))\n",
    "layer6 = Dense(20, activation = 'relu')(latent_inputs)\n",
    "layer7 = Dense(30, activation = 'relu')(layer6)\n",
    "layer8 = Dense(50, activation = 'relu')(layer7)\n",
    "layer9 = Dense(100, activation = 'relu')(layer8)\n",
    "layer10 = Dense(200, activation = 'relu')(layer9)\n",
    "decoder_outputs = Dense(6, activation = 'sigmoid')(layer10)\n",
    "decoder = keras.Model(latent_inputs, decoder_outputs, name=\"decoder\")\n",
    "decoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3350a964",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAE(Model):\n",
    "    def __init__(self, encoder, sampler, decoder, beta = 1, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.encoder = encoder\n",
    "        self.sampler = sampler\n",
    "        self.decoder = decoder\n",
    "        self.beta = beta\n",
    "        self.total_loss_tracker = keras.metrics.Mean(name=\"total_loss\")\n",
    "        self.reconstruction_loss_tracker = keras.metrics.Mean(\n",
    "            name=\"reconstruction_loss\"\n",
    "        )\n",
    "        self.kl_loss_tracker = keras.metrics.Mean(name=\"kl_loss\")\n",
    "\n",
    "    @property\n",
    "    def metrics(self):\n",
    "        return [\n",
    "            self.total_loss_tracker,\n",
    "            self.reconstruction_loss_tracker,\n",
    "            self.kl_loss_tracker,\n",
    "        ]\n",
    "\n",
    "    def train_step(self, data):\n",
    "        data = data[0]\n",
    "        with tf.GradientTape() as tape:\n",
    "            z_mean, z_log_sigma = self.encoder(data)\n",
    "            z_log_var = 2 * z_log_sigma\n",
    "            z = self.sampler([z_mean, z_log_var])\n",
    "            reconstruction = self.decoder(z)\n",
    "            reconstruction_loss = tf.reduce_mean(keras.losses.binary_crossentropy(data, reconstruction))\n",
    "            kl_loss = -0.5 * (1 + z_log_var - tf.square(z_mean) - tf.exp(z_log_var))\n",
    "            kl_loss = tf.reduce_mean(kl_loss)\n",
    "            total_loss = reconstruction_loss + self.beta * kl_loss\n",
    "        grads = tape.gradient(total_loss, self.trainable_weights)\n",
    "        self.optimizer.apply_gradients(zip(grads, self.trainable_weights))\n",
    "        self.total_loss_tracker.update_state(total_loss)\n",
    "        self.reconstruction_loss_tracker.update_state(reconstruction_loss)\n",
    "        self.kl_loss_tracker.update_state(kl_loss)\n",
    "        return {\n",
    "            \"loss\": self.total_loss_tracker.result(),\n",
    "            \"reconstruction_loss\": self.reconstruction_loss_tracker.result(),\n",
    "            \"kl_loss\": self.kl_loss_tracker.result(),\n",
    "        }\n",
    "    def test_step(self, data):\n",
    "        if isinstance(data, tuple):\n",
    "            #print(f\"Ran. data: {data}\")\n",
    "            data = data[0]\n",
    "\n",
    "        z_mean, z_log_sigma = self.encoder(data)\n",
    "        z_log_var = 2 * z_log_sigma\n",
    "        z = self.sampler([z_mean, z_log_var])\n",
    "        reconstruction = self.decoder(z)\n",
    "        reconstruction_loss = tf.reduce_mean(keras.losses.binary_crossentropy(data, reconstruction))\n",
    "        kl_loss = -0.5 * (1 + z_log_var - tf.square(z_mean) - tf.exp(z_log_var))\n",
    "        kl_loss = tf.reduce_mean(kl_loss)\n",
    "        total_loss = reconstruction_loss + self.beta * kl_loss\n",
    "        return {\n",
    "            \"loss\": total_loss,\n",
    "            \"reconstruction_loss\": reconstruction_loss,\n",
    "            \"kl_loss\": kl_loss,\n",
    "        }\n",
    "    \n",
    "    \n",
    "    def call(self, data):\n",
    "        z_mean, z_log_sigma = self.encoder(data)\n",
    "        z_log_var = 2 * z_log_sigma\n",
    "        z = self.sampler([z_mean, z_log_var])\n",
    "        reconstruction = self.decoder(z)\n",
    "        reconstruction_loss = tf.reduce_mean(keras.losses.binary_crossentropy(data, reconstruction))\n",
    "        kl_loss = -0.5 * (1 + z_log_var - tf.square(z_mean) - tf.exp(z_log_var))\n",
    "        kl_loss = tf.reduce_mean(kl_loss)\n",
    "        total_loss = reconstruction_loss + self.beta * kl_loss\n",
    "        self.add_metric(kl_loss, name='kl_loss', aggregation='mean')\n",
    "        self.add_metric(total_loss, name='total_loss', aggregation='mean')\n",
    "        self.add_metric(reconstruction_loss, name='reconstruction_loss', aggregation='mean')\n",
    "        return reconstruction\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34bd3cdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "vae = VAE(encoder, sampler, decoder, beta = 0.01)\n",
    "vae.compile(optimizer = 'adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74da16be",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"C:/Users/ernest.liu/Documents/git/Morphine-22-23/ML/Model/weights/\")\n",
    "model_name = \"vae_fixed_loss_beta_0_01\"\n",
    "vae.load_weights(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae0d295b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_predictions = vae.predict(train_data_norm)\n",
    "test_predictions = vae.predict(test_data_norm)\n",
    "valid_predictions = vae.predict(valid_data_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "516b59e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_mae = np.sum(np.abs(valid_data_norm - valid_predictions), axis = 1)\n",
    "best_threshold = np.quantile(valid_mae, 0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7be0622f",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1d0af01",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf33aa02",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "266214cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler.transform(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa92967e",
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(scaler, open('scaler.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dde2f0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4ed2a0a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0db66bb3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec8abe5a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7def71b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe18d62b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "110373a9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ccc2ae0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9192bf6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"C:/Users/ernest.liu/Documents/git/Morphine-22-23/ML/Docker/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc1524cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import firebase_admin\n",
    "from firebase_admin import db\n",
    "from datetime import datetime\n",
    "import time\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Connection to Firebase\n",
    "cred_obj = firebase_admin.credentials.Certificate(\"../Morphine2.json\")\n",
    "default_app = firebase_admin.initialize_app(cred_obj, {\n",
    "  'databaseURL':'https://morphine-64cdd-default-rtdb.asia-southeast1.firebasedatabase.app/'\n",
    "  })\n",
    "USERS_DATA = db.reference(\"/Users Data/Token UID:XvIeVwC7M0QN0qW15FNYO2e5BJ93\")\n",
    "FIREBASE_PREDICTION_PATH = db.reference(\"/Users Data/Token UID:XvIeVwC7M0QN0qW15FNYO2e5BJ93/Split Circuit/MPU6050/MPU6050 Fall/\")\n",
    "\n",
    "# Process GPS Data\n",
    "GPS_KEYWORDS_LIST = ['Latitude: ', '(*10^-7) Longitude: ', '(*10^-7) Altitude: ', '(mm) Satellite-in-view: ', 'timing for this set: ']\n",
    "GPS_KEYWORDS_LENGTH = [len(keyword) for keyword in GPS_KEYWORDS_LIST]\n",
    "\n",
    "def process_gps_datapoints(gps_datapoints):\n",
    "    \"\"\"\n",
    "    gps_datapoints = gps_data['GPS Datapoints']\n",
    "    \"\"\"\n",
    "    gps_datapoints = gps_datapoints[0]\n",
    "    indices = [gps_datapoints.find(keyword) for keyword in GPS_KEYWORDS_LIST]\n",
    "    # Latitude, Longtiude, Altitude, Satellite-in-view, timing for this set\n",
    "    gps_datapoints_list = []\n",
    "    for i, index in enumerate(indices):\n",
    "        if i == len(indices) - 1:\n",
    "            gps_datapoints_list.append(float(gps_datapoints[index + GPS_KEYWORDS_LENGTH[i]:].strip()))\n",
    "        else:\n",
    "            gps_datapoints_list.append(float(gps_datapoints[index + GPS_KEYWORDS_LENGTH[i]:indices[i+1]].strip()))\n",
    "    return gps_datapoints_list\n",
    "\n",
    "def process_gps(gps):\n",
    "    \"\"\"\n",
    "    gps = data['Split Circuit']['GPS']\n",
    "    \"\"\"\n",
    "    processed_gps_data = []\n",
    "    gps_accounter = gps['GPS Accounter']\n",
    "    gps_datapoints = gps['GPS Datapoints']\n",
    "    gps_loopSpeedArr = gps['GPS LoopSpeedArr'][0]\n",
    "    gps_uploadSpeedArr = gps['GPS UploadSpeedArr'][0]\n",
    "    \n",
    "    processed_gps_datapoints = process_gps_datapoints(gps_datapoints)\n",
    "    \n",
    "    processed_gps_data.append(gps_accounter)\n",
    "    processed_gps_data.extend(processed_gps_datapoints)\n",
    "    processed_gps_data.extend([gps_loopSpeedArr, gps_uploadSpeedArr])\n",
    "    \n",
    "    new_gps_df = pd.DataFrame([processed_gps_data],\n",
    "                             columns = ['accounter', 'latitude', 'longitude', 'altitude', 'satelliteInView', 'timingForThisSet', 'LoopSpeed', 'UploadSpeed'])\n",
    "    return new_gps_df\n",
    "\n",
    "# process MPU6050\n",
    "MPU6050_KEYWORDS = ['Ax: ', 'Ay: ', 'Az: ', 'gx: ', 'gy: ', 'gz: ', 'temp: ', 'timing for this set: ']\n",
    "MPU6050_KEYWORDS_LENGTH = [len(x) for x in MPU6050_KEYWORDS]\n",
    "\n",
    "## function to process one datapoint\n",
    "def process_one_set_of_datapoint(output_set):\n",
    "    indexes = [output_set.find(keyword) for keyword in MPU6050_KEYWORDS]\n",
    "    df_row = []\n",
    "    curr_data_index = int(output_set[:indexes[0]].strip())\n",
    "    df_row.append(curr_data_index) # append in the index of the new input\n",
    "    for i, index in enumerate(indexes):\n",
    "        if i == len(indexes) - 1:\n",
    "            x = float(output_set[index+MPU6050_KEYWORDS_LENGTH[i]:].strip())\n",
    "            df_row.append(x)\n",
    "        else:\n",
    "            x = float(output_set[index+MPU6050_KEYWORDS_LENGTH[i]: indexes[i+1]].strip())\n",
    "            df_row.append(x)\n",
    "    return df_row\n",
    "\n",
    "def process_mpu6050(mpu6050, timeDifference):\n",
    "    \"\"\"\n",
    "    mpu6050_output = split_circuit_data['MPU6050']\n",
    "    \"\"\"\n",
    "    mpu6050_df = pd.DataFrame(columns = ['accounter', 'LoopSpeedArr', 'UploadSpeedArr', 'set_index', 'Ax', 'Ay', 'Az', 'gx', 'gy', 'gz', 'temp', 'timingForThisSet', 'timeDifference'])\n",
    "    accounter = mpu6050['MPU6050 Accounter']\n",
    "    mpu6050_datapoints = mpu6050['MPU6050 Datapoints'][0]\n",
    "    mpu6050_loopSpeedArr = mpu6050['MPU6050 LoopSpeedArr'][0]\n",
    "    mpu6050_uploadSpeedArr = mpu6050['MPU6050 UploadSpeedArr'][0]\n",
    "    mpu6050_output_sets = mpu6050_datapoints.split('Set: ')[1:]\n",
    "    \n",
    "    for output in mpu6050_output_sets:\n",
    "        data = [accounter, mpu6050_loopSpeedArr,mpu6050_uploadSpeedArr]\n",
    "        datapoint = process_one_set_of_datapoint(output)\n",
    "        data.extend(datapoint)\n",
    "        data.append(timeDifference)\n",
    "        new_df = pd.DataFrame([data], \n",
    "                              columns = ['accounter', 'LoopSpeedArr', 'UploadSpeedArr', 'set_index', 'Ax', 'Ay', 'Az', 'gx', 'gy', 'gz', 'temp', 'timingForThisSet', 'timeDifference'])\n",
    "        mpu6050_df = pd.concat([mpu6050_df, new_df], ignore_index = True)\n",
    "    return mpu6050_df\n",
    "\n",
    "# overall function to read split circuit\n",
    "def process_split_ciruit_data(split_circuit_data):\n",
    "    \"\"\"\n",
    "    split_circuit_data = data['Split Circuit']\n",
    "    \"\"\"\n",
    "    gps_df = pd.DataFrame(columns = ['accounter', 'latitude', 'longitude', 'altitude', 'satelliteInView', 'timingForThisSet', 'LoopSpeed', 'UploadSpeed'])\n",
    "    mpu6050_df = pd.DataFrame(columns = ['accounter', 'LoopSpeedArr', 'UploadSpeedArr', 'set_index', 'Ax', 'Ay', 'Az', 'gx', 'gy', 'gz', 'temp', 'timingForThisSet', 'timeDifference'])\n",
    "\n",
    "    keys = split_circuit_data.keys()\n",
    "    for key in keys:\n",
    "        if key == 'GPS':\n",
    "            gps_data = split_circuit_data['GPS']\n",
    "            processed_gps_data = process_gps(gps_data)\n",
    "            gps_df = pd.concat([gps_df, processed_gps_data])\n",
    "            print('Extracted GPS Data')\n",
    "            \n",
    "        elif key == 'GPS Button':\n",
    "            print(\"GPS Button:\", split_circuit_data['GPS Button'])\n",
    "            \n",
    "        elif key == 'MPU6050':\n",
    "            mpu6050_output = split_circuit_data['MPU6050']\n",
    "            processed_mpu6050_output = process_mpu6050(mpu6050_output, None)\n",
    "            mpu6050_df = pd.concat([mpu6050_df, processed_mpu6050_output])\n",
    "            print('Extracted MPU6050 Data')\n",
    "    return gps_df, mpu6050_df\n",
    "\n",
    "# MAIN FUNCTION to read data from Google Firebase\n",
    "def read_data():\n",
    "    data = USERS_DATA.get()\n",
    "    split_circuit_data = data['Split Circuit']\n",
    "    gps_df, mpu6050_df = process_split_ciruit_data(split_circuit_data)\n",
    "    return gps_df, mpu6050_df\n",
    "\n",
    "def predict(wave_data, t1=8.6396, t2=0.5):\n",
    "    \"\"\"\n",
    "    Actual:\n",
    "    This function make use of the autoencoder model to predict one wave of data - 20 datapoints.\n",
    "    The model will predict whether each of this 20 datapoints is ADL or Fall from the reconstruction error threshold, t1.\n",
    "    If the number of datapoints in a wave is more than a certain threshold, t2, we will then classify it as Fall, else ADL.\n",
    "\n",
    "    For now:\n",
    "    t1 is threshold to classify one datapoint based on the y-axis of accelerometer \n",
    "    t2 is threshold to classify a wave - 0.5 i.e. if >= 50% of the datapoints are anomalous, this wave will be classified as fall\n",
    "    \"\"\"\n",
    "    return np.sum(np.abs(wave_data.Ay) > t1) >= t2 * wave_data.shape[0]\n",
    "\n",
    "def write_to_firebase(prediction, prediction_path=FIREBASE_PREDICTION_PATH):\n",
    "    \"\"\" Write results to firebase \"\"\"\n",
    "    if prediction.lower() == \"fall detected\":\n",
    "        prediction_path.update({\"0\":\"Fall Detected\"})\n",
    "    elif prediction.lower() == \"normal\":\n",
    "        prediction_path.update({\"0\":\"Normal\"})\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     while True:\n",
    "#         gps_df, mpu6050_df = read_data()\n",
    "#         print(\"GPS and MPU6050 Dataframe Shape:\", gps_df.shape, mpu6050_df.shape)\n",
    "#         # prediction = predict(wave_data=mpu6050_df.values)\n",
    "#         prediction = \"Fall detected\"\n",
    "#         print(\"Prediction:\", prediction.title())\n",
    "#         # write_to_firebase(prediction)\n",
    "#         print(\"Updated Firebase!\")\n",
    "#         print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "594e9cee",
   "metadata": {},
   "outputs": [],
   "source": [
    "gps_df, mpu6050_df = read_data()\n",
    "test_data = read_data()[1][[\"Ax\", \"Ay\", \"Az\", \"gx\", \"gy\", \"gz\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cfc64f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler.transform(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57f4d64c",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de2b06af",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce1d62ef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf9017f2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dbcdf57",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
